{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-933b504e-1de2-422e-9fac-6cdfbf6cfb56",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# Intelligente systemer: Q-læring\n",
    "Velkommen til denne notebooken som inngår i Høyskolen Kristianias emne «Utvikling og anvendelse av intelligente systemer», modul 6, «Forsterkende læring».\n",
    "\n",
    "## Hvordan du bruker denne notebooken \n",
    "Dette er en Jupyter notebook med tekst og Python-kode som du kjører ved å trykke shift-enter. Hensikten er å bli kjent med relevante konsepter i praksis, ikke nødvendigvis å forstå alle detaljer i koden. Ta deg god tid til å lese teksten, se gjennom koden, kjøre kodecellene (pass på riktig rekkefølge), studere resultatene, og tenke gjennom hva vi gjør, hvorfor og hvordan.\n",
    "\n",
    "Underveis i teksten vil du finne referanser til Canvas-oppgaver _i kursiv_. Gå til Canvas og besvar spørsmålene der før du går videre i notebooken.\n",
    "\n",
    "## Problemstilling: Autonom robot til fiolinfabrikk\n",
    "Vi skal konstruere en autonom robot til en fiolinfabrikk. I fabrikken skal roboter bringe riktige deler til  fiolinmakerne, i riktig rekkefølge. Delene er plassert på 9 ulike lagerplasser, kalt L1–L9. Disse er vist i figuren under. L6 har en spesiell rolle: Det er her fiolinkroppene oppbevares, og det er hit delene skal bringes.\n",
    "\n",
    "Vi skal prøve oss på en enkel Q-læringsmodell hvor verdien av hver tilstand og handling beskrives i en tabell. Dermed er ikke dette dyp læring. Hvis vi skulle ha brukt dyp læring, måtte vi ha erstattet tabellen med et nevralt nettverk og oppdatert det hver gang det ble samlet inn ny erfaring.\n",
    "\n",
    "Eksemplet i denne notebooken ligger tett opp til denne bloggen: https://blog.floydhub.com/an-introduction-to-q-learning-reinforcement-learning/\n",
    "\n",
    "![Environment. Figure from Sayak Paul at blog.floydhub.com](./figures/environment.png)\n",
    "\n",
    "Oppgaven er å trene roboten (agenten) til å finne den korteste ruten mellom alle lokasjoner på lageret (miljøet). Handlingene som roboten kan utføre, er å bevege seg direkte til ikke-blokkerte nabo-lagerplasser. Det er ikke et fast antall mulige handlinger fra hver tilstand: Fra L4 kan roboten kun velge å gå til L7, mens den fra L7 kan velge å gå til enten L4 eller L8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-34beb6ad-b9d1-4d89-a851-530706a7a95e",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "_Oppgave: Svar på flervalgsspørsmål 1 (vist nedenfor) i Canvas._\n",
    "\n",
    "1. _Hva er den korteste lovlige ruten fra L9 til L6?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oppsett"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "00000-ecf94482-4c91-408a-87c9-6852791bc2bf",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1639057891960,
    "source_hash": "25f31e65",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "00003-18593cc3-d994-4f87-bd97-20fd0b04b10c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1639057773575,
    "source_hash": "ea60f014",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the states as numbers for convenience later\n",
    "location_to_state = {\n",
    "    'L1' : 0,\n",
    "    'L2' : 1,\n",
    "    'L3' : 2,\n",
    "    'L4' : 3,\n",
    "    'L5' : 4,\n",
    "    'L6' : 5,\n",
    "    'L7' : 6,\n",
    "    'L8' : 7,\n",
    "    'L9' : 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "00004-5dce5e7b-9b18-4bd1-acbe-48362018e75f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4,
    "execution_start": 1639057773594,
    "source_hash": "78f66a27",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the actions as the location to go to\n",
    "actions = [0,1,2,3,4,5,6,7,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-86849e9c-1ceb-4ede-8430-f39ac5863c0f",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Det gis belønning til roboten når tilstanden den befinner seg i er direkte forbundet med destinasjons-tilstanden. Det vil si at hvis den står på L8, gis det belønning hvis destinasjonen er L9, men ikke hvis destinasjonen er L6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "00005-e5aad47d-2dc6-409d-b647-e5d29ea863a6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1639057773607,
    "source_hash": "27ade38d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the rewards\n",
    "rewards = np.array([[0,1,0,0,0,0,0,0,0],\n",
    "                    [1,0,1,0,1,0,0,0,0],\n",
    "                    [0,1,0,0,0,1,0,0,0],\n",
    "                    [0,0,0,0,0,0,1,0,0],\n",
    "                    [0,1,0,0,0,0,0,1,0],\n",
    "                    [0,0,1,0,0,0,0,0,0],\n",
    "                    [0,0,0,1,0,0,0,1,0],\n",
    "                    [0,0,0,0,1,0,1,0,1],\n",
    "                    [0,0,0,0,0,0,0,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-2ec0d216-e77d-49f8-81c4-dedbe3189489",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Her er det faktisk ingen fysisk barriere i systemet, MEN roboten oppfordres til å nedprioritere ferdsel mellom visse plasser fordi den ikke får belønning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-8fe2dbeb-9850-4b29-9f2d-6442099a7984",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "_Oppgave: Svar på flervalgsspørsmål 2 (vist nedenfor) i Canvas._\n",
    "\n",
    "2. _Roboten får kun belønning hvis den ikke krysser en sperret rute. Hvordan kan man gjøre slike hindringer enda tydeligere for roboten i dette oppsettet?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "00010-11db8c9e-3954-4a69-bf64-1cc60d0c2c48",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6954642,
    "execution_start": 1639057773663,
    "source_hash": "c1c8057b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "gamma = 0.75 # Discount factor \n",
    "alpha = 0.9 # Learning rate \n",
    "# Initializing Q-Values\n",
    "Q = np.array(np.zeros([9,9]))\n",
    "#iterations = 3\n",
    "iterations = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-c3ab7ad5-bbb9-4349-94c3-0c3f1ac6b826",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "_Oppgave: Svar på flervalgsspørsmål 3 (vist nedenfor) i Canvas._\n",
    "\n",
    "3. _Hva betyr diskonteringsfaktoren?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-7983e2f4-c77c-41ec-be64-5e3ac4250d9d",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Kode for å løse problemet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-ffb9dc8a-5902-45b5-b14c-8972653c733e",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "For å forenkle treningsprosessen og kunne eksperimentere litt, lager vi en klasse kalt `QAgent()`. Klassen består av: \n",
    "\n",
    "- **\\_\\_init__**: Her initialiseres klassen med miljø, handlinger og belønninger. Nødvendig input er: \n",
    "\n",
    "    - **gamma**: Diskonteringsfaktoren i Bellmans ligning. \n",
    "    - **alpha**: Læringsraten i Bellmans ligning. \n",
    "    - **location_to_state**: Dictionary som oversetter mellom tilstand og indeksnummer for tilstanden. \n",
    "    - **actions**: Array av mulige handlinger. \n",
    "    - **rewards**: 2D-array med belønninger som angir hvilke lokasjoner som er 'direkte' forbundet. \n",
    "    - **state_to_location**: Dictionary som oversetter mellom indeksnummer for tilstanden og navnet. \n",
    "    - **Q**: Q-verdier. Enten et tomt array eller en pre-trent modell. \n",
    "\n",
    "\n",
    "- **training**: Funksjon som trener roboten. Den tar som input: \n",
    "    - **start_location**: Start-lokasjon. \n",
    "    - **end_location**: Mål-lokasjon. \n",
    "    - **iterations**: Antall ganger modellen skal forsøke å forbedre seg. \n",
    "\n",
    "    For å kommunisere til roboten at det er mål-tilstanden vi ønsker å oppnå, er første steg i treningen å øke verdien til belønningen på nettopp målposisjonen. For hver iterasjon velger algoritmen et tilfeldig startsted og lister de gyldige handlingene. Roboten utforsker en tilfeldig handling fra startstedet, før den beregner belønning og Q-verdi av handlingen og oppdaterer Q-tabellen. Til slutt kaller den `get_optimal_route`, som bruker Q-tabellen til å angi ruten med høyest forventet belønning.\n",
    "    \n",
    "- **get_optimal_route**: Funksjon som gir roboten den optimale ruten mellom to lokasjoner basert på grådig utnyttelse av Q-tabellen. Det vil si at roboten alltid skal bevege seg mot høyere Q-verdi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "00011-feb8c567-3997-4f56-801f-12be054df5c2",
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     173,
     250,
     611,
     420
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1639060468120,
    "source_hash": "25e63fbc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QAgent():\n",
    "    \n",
    "    # Initialize alpha, gamma, states, actions, rewards, and Q-values\n",
    "    def __init__(self, alpha, gamma, location_to_state, actions, rewards, Q):\n",
    "        \n",
    "        self.gamma = gamma  \n",
    "        self.alpha = alpha \n",
    "        \n",
    "        self.location_to_state = location_to_state\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        # Inverse mapping of indices to locations\n",
    "        self.state_to_location = dict((state,location) \n",
    "            for location,state in location_to_state.items())\n",
    "        self.Q = Q\n",
    "        \n",
    "    # Training the robot in the environment\n",
    "    def training(self, start_location, end_location, iterations, verbose=False):\n",
    "        \n",
    "        rewards_new = np.copy(self.rewards)\n",
    "        \n",
    "        ending_state = self.location_to_state[end_location]\n",
    "        rewards_new[ending_state, ending_state] = 999   # Communicate to robot that end state is special\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            current_state = np.random.choice(list(location_to_state.values()))  # Randomly sample starting point\n",
    "            playable_actions = []\n",
    "\n",
    "            # Iterate through the new rewards matrix and get the actions > 0\n",
    "            for j in range(9):\n",
    "                if rewards_new[current_state,j] > 0:\n",
    "                    playable_actions.append(j)  # possible actions from starting location\n",
    "    \n",
    "            next_state = np.random.choice(playable_actions)  # pick random action among those possible\n",
    "            TD = rewards_new[current_state, next_state] + \\\n",
    "                    self.gamma * self.Q[next_state, np.argmax(self.Q[next_state,:])] \\\n",
    "                        - self.Q[current_state, next_state]  # Bellman's equation for the Q-value\n",
    "            # update Q-table with best value multiplied by learning rate\n",
    "            self.Q[current_state, next_state] += self.alpha * TD  \n",
    "\n",
    "            if verbose==True:\n",
    "                print(f'Iteration {i}: {current_state} to {ending_state}, next state {next_state}, current Q={TD}')\n",
    "\n",
    "        # Initialize the optimal route with the starting location\n",
    "        route = [start_location]\n",
    "        # We do not know the next move of the robot, so we set the next location to the starting location\n",
    "        next_location = start_location\n",
    "\n",
    "        sns.heatmap(self.Q)\n",
    "        plt.title(\"Q-table\", fontsize=10)\n",
    "        plt.show()\n",
    "\n",
    "        # Get the route \n",
    "        self.get_optimal_route(start_location, end_location, next_location, route, self.Q)\n",
    "        \n",
    "    # Get the optimal route\n",
    "    def get_optimal_route(self, start_location, end_location, next_location, route, Q):\n",
    "        \n",
    "        while(next_location != end_location):\n",
    "            starting_state = self.location_to_state[start_location]\n",
    "            # The possible action with the highest immediate and future expected \n",
    "            # reward is given by the maximal Q-value\n",
    "            next_state = np.argmax(Q[starting_state,])  \n",
    "            next_location = self.state_to_location[next_state]\n",
    "            route.append(next_location)\n",
    "            start_location = next_location\n",
    "\n",
    "            if len(route) > rewards.size:\n",
    "                print('Q-matrix too little explored, increase number of iterations')\n",
    "                break\n",
    "        \n",
    "        print(route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "cell_id": "00016-ae273bae-fc16-4ce8-89da-988651a676d3",
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     null,
     264
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 273,
    "execution_start": 1639060475260,
    "source_hash": "cc00c853",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: 1 to 5, next state 2, current Q=1.0\n",
      "Iteration 1: 3 to 5, next state 6, current Q=1.0\n",
      "Iteration 2: 6 to 5, next state 7, current Q=1.0\n",
      "Iteration 3: 1 to 5, next state 4, current Q=1.0\n",
      "Iteration 4: 8 to 5, next state 7, current Q=1.0\n",
      "Iteration 5: 6 to 5, next state 7, current Q=0.09999999999999998\n",
      "Iteration 6: 5 to 5, next state 5, current Q=999.0\n",
      "Iteration 7: 4 to 5, next state 1, current Q=1.675\n",
      "Iteration 8: 7 to 5, next state 8, current Q=1.675\n",
      "Iteration 9: 1 to 5, next state 2, current Q=0.09999999999999998\n",
      "Iteration 10: 0 to 5, next state 1, current Q=1.7425\n",
      "Iteration 11: 7 to 5, next state 6, current Q=1.7425\n",
      "Iteration 12: 4 to 5, next state 1, current Q=0.23499999999999988\n",
      "Iteration 13: 0 to 5, next state 1, current Q=0.17425000000000002\n",
      "Iteration 14: 3 to 5, next state 6, current Q=0.8424999999999999\n",
      "Iteration 15: 3 to 5, next state 6, current Q=0.08424999999999994\n",
      "Iteration 16: 4 to 5, next state 1, current Q=0.023500000000000076\n",
      "Iteration 17: 6 to 5, next state 3, current Q=2.30055625\n",
      "Iteration 18: 3 to 5, next state 6, current Q=0.8188004687499999\n",
      "Iteration 19: 8 to 5, next state 7, current Q=1.2761875000000003\n",
      "Iteration 20: 5 to 5, next state 5, current Q=774.225\n",
      "Iteration 21: 3 to 5, next state 6, current Q=0.08188004687499983\n",
      "Iteration 22: 8 to 5, next state 7, current Q=0.1276187499999999\n",
      "Iteration 23: 3 to 5, next state 6, current Q=0.008188004687499806\n",
      "Iteration 24: 6 to 5, next state 7, current Q=1.1861875000000002\n",
      "Iteration 25: 2 to 5, next state 5, current Q=1197.926875\n",
      "Iteration 26: 1 to 5, next state 4, current Q=1.4051125\n",
      "Iteration 27: 2 to 5, next state 1, current Q=2.6234509375\n",
      "Iteration 28: 1 to 5, next state 0, current Q=2.29380625\n",
      "Iteration 29: 2 to 5, next state 1, current Q=0.26234509375000004\n",
      "Iteration 30: 2 to 5, next state 5, current Q=119.79268750000006\n",
      "Iteration 31: 0 to 5, next state 1, current Q=0.8983759375\n",
      "Iteration 32: 0 to 5, next state 1, current Q=0.08983759375\n",
      "Iteration 33: 6 to 5, next state 7, current Q=0.11861875\n",
      "Iteration 34: 1 to 5, next state 2, current Q=889.4707046875001\n",
      "Iteration 35: 2 to 5, next state 1, current Q=599.5380092359376\n",
      "Iteration 36: 3 to 5, next state 6, current Q=0.07118755046875025\n",
      "Iteration 37: 4 to 5, next state 1, current Q=600.3950756640626\n",
      "Iteration 38: 3 to 5, next state 6, current Q=0.0071187550468749805\n",
      "Iteration 39: 0 to 5, next state 1, current Q=599.5207584859376\n",
      "Iteration 40: 2 to 5, next state 5, current Q=11.979268750000074\n",
      "Iteration 41: 7 to 5, next state 8, current Q=1.1150692187500002\n",
      "Iteration 42: 5 to 5, next state 2, current Q=898.54671109375\n",
      "Iteration 43: 5 to 5, next state 2, current Q=89.85467110937498\n",
      "Iteration 44: 3 to 5, next state 6, current Q=0.0007118755046873204\n",
      "Iteration 45: 7 to 5, next state 4, current Q=407.57178857324226\n",
      "Iteration 46: 4 to 5, next state 1, current Q=60.0395075664062\n",
      "Iteration 47: 7 to 5, next state 4, current Q=81.28384646464843\n",
      "Iteration 48: 0 to 5, next state 1, current Q=59.95207584859372\n",
      "Iteration 49: 8 to 5, next state 7, current Q=328.8141280255762\n",
      "Iteration 50: 4 to 5, next state 1, current Q=6.00395075664062\n",
      "Iteration 51: 5 to 5, next state 2, current Q=8.98546711093752\n",
      "Iteration 52: 8 to 5, next state 7, current Q=32.881412802557634\n",
      "Iteration 53: 7 to 5, next state 4, current Q=12.181051407197174\n",
      "Iteration 54: 2 to 5, next state 1, current Q=59.95380092359369\n",
      "Iteration 55: 3 to 5, next state 6, current Q=7.118755046864322e-05\n",
      "Iteration 56: 2 to 5, next state 1, current Q=5.995380092359369\n",
      "Iteration 57: 2 to 5, next state 1, current Q=0.599538009235971\n",
      "Iteration 58: 1 to 5, next state 4, current Q=449.98652169129883\n",
      "Iteration 59: 0 to 5, next state 1, current Q=5.995207584859372\n",
      "Iteration 60: 1 to 5, next state 0, current Q=450.0873530541825\n",
      "Iteration 61: 4 to 5, next state 1, current Q=0.6003950756640961\n",
      "Iteration 62: 7 to 5, next state 8, current Q=244.25599698086532\n",
      "Iteration 63: 1 to 5, next state 2, current Q=97.03307687499989\n",
      "Iteration 64: 3 to 5, next state 6, current Q=7.1187550467755045e-06\n",
      "Iteration 65: 2 to 5, next state 1, current Q=65.5572806915485\n",
      "Iteration 66: 7 to 5, next state 6, current Q=1.05499421875\n",
      "Iteration 67: 2 to 5, next state 5, current Q=1.197926875000121\n",
      "Iteration 68: 0 to 5, next state 1, current Q=66.09684764911083\n",
      "Iteration 69: 0 to 5, next state 1, current Q=6.609684764911094\n",
      "Iteration 70: 0 to 5, next state 1, current Q=0.6609684764911208\n",
      "Iteration 71: 5 to 5, next state 5, current Q=600.024375\n",
      "Iteration 72: 2 to 5, next state 5, current Q=405.1362458125002\n",
      "Iteration 73: 1 to 5, next state 2, current Q=283.97887425156273\n",
      "Iteration 74: 1 to 5, next state 2, current Q=28.39788742515634\n",
      "Iteration 75: 7 to 5, next state 8, current Q=24.42559969808653\n",
      "Iteration 76: 0 to 5, next state 1, current Q=210.92041097943456\n",
      "Iteration 77: 3 to 5, next state 6, current Q=7.118755047663683e-07\n",
      "Iteration 78: 0 to 5, next state 1, current Q=21.09204109794348\n",
      "Iteration 79: 0 to 5, next state 1, current Q=2.109204109794291\n",
      "Iteration 80: 8 to 5, next state 7, current Q=11.510350980113856\n",
      "Iteration 81: 3 to 5, next state 6, current Q=7.118755052104575e-08\n",
      "Iteration 82: 4 to 5, next state 1, current Q=276.4116805299768\n",
      "Iteration 83: 1 to 5, next state 2, current Q=2.839788742515566\n",
      "Iteration 84: 2 to 5, next state 1, current Q=219.32689960213838\n",
      "Iteration 85: 1 to 5, next state 0, current Q=252.5639163328559\n",
      "Iteration 86: 8 to 5, next state 7, current Q=1.1510350980113913\n",
      "Iteration 87: 1 to 5, next state 2, current Q=0.2839788742514884\n",
      "Iteration 88: 5 to 5, next state 5, current Q=465.01889062500004\n",
      "Iteration 89: 0 to 5, next state 1, current Q=2.3194635522971794\n",
      "Iteration 90: 4 to 5, next state 7, current Q=339.1997633504343\n",
      "Iteration 91: 4 to 5, next state 1, current Q=29.749711194315523\n",
      "Iteration 92: 4 to 5, next state 7, current Q=33.91997633504343\n",
      "Iteration 93: 7 to 5, next state 8, current Q=10.988995572543217\n",
      "Iteration 94: 7 to 5, next state 6, current Q=0.10549942187499983\n",
      "Iteration 95: 1 to 5, next state 0, current Q=26.822029531086173\n",
      "Iteration 96: 2 to 5, next state 5, current Q=354.40137575312474\n",
      "Iteration 97: 2 to 5, next state 1, current Q=22.12437570033353\n",
      "Iteration 98: 7 to 5, next state 6, current Q=0.010549942187500161\n",
      "Iteration 99: 0 to 5, next state 1, current Q=0.23194635522975204\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L9', 'L8', 'L5', 'L2', 'L3', 'L6']\n"
     ]
    }
   ],
   "source": [
    "agent = QAgent(alpha, gamma, location_to_state, actions, rewards, Q)\n",
    "agent.training('L9','L6', iterations, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00016-e88e413b-dca0-4fca-8aaa-9775d66da6e5",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "_Oppgave: Svar på flervalgsspørsmål 4–7 (vist nedenfor) i Canvas._\n",
    "\n",
    "4. _Hvor mange iterasjoner bør man minimum trene?_\n",
    "5. _Øk antall iterasjoner til 100. Hva foreslår algoritmen da som den optimale ruten mellom L9 og L6?_\n",
    "6. _I denne algoritmen utforsker vi tilfeldig. Hvordan kunne vi ha implementert en mer effektiv utforskning?_\n",
    "7. _Hva må vi gjøre hvis vi ønsker å konvertere algoritmen til dyp forsterkende læring?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Oppgave: Svar på «Vi spør – du svarer»-spørsmål 1 (vist nedenfor) i Canvas._\n",
    "\n",
    "1. _Beskriv kort hva som måtte endres dersom fabrikken kjøpte nabobygget som hadde ytterligere ni lokasjoner._"
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "c9716a1b-c213-4f85-9f62-ab204c183f4d",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
